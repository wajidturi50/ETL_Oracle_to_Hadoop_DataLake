# ETL PIPELINE (TAKE DATA FROM ORACLE DB AND INGEST INTO HADOOP)

![Project Image](https://github.com/wajidturi50/ETL_Oracle_to_Hadoop_DataLake/raw/main/Final%20Results.png)

## Explanation

Welcome to the Oracle to Hadoop Data Lake ETL project! This powerful ETL (Extract, Transform, Load) solution aims to efficiently extract data from multiple tables in an Oracle database and ingest it into a Hadoop data lake using PySpark.

The project utilizes PySpark, a Python library that provides an interface for Apache Spark, a fast and general-purpose cluster computing system. PySpark enables seamless integration with Hadoop's distributed file system and allows us to process and transform large datasets in parallel.

To enhance the ETL process, the project incorporates configuration with Kudu nodes and Impala. Kudu is a columnar storage manager for Hadoop that enables fast analytics on fast data. Impala, on the other hand, is a massively parallel processing (MPP) SQL query engine for Hadoop that provides real-time, interactive SQL queries on large datasets.


## Contact

If you have any questions or suggestions, feel free to reach out to me at [wajidturi7@gmail.com].

Thank you for visiting this repository and happy coding!
